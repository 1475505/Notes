
# 事务

[此部分已简单耦合至数据库笔记中](Database/db.md)

| 并发问题       | 描述 | 解决方案     |
| ---------- | ---- | ------------ |
| 脏写       |      | 行级锁       |
| 脏读       |      | 语句级MVCC   |
| 不可重复读 |      | 事务级MVCC   |
| 更新丢失   |      | `for update` |
| 写倾斜     |      | 可串行化     |
| 幻读           |      | 索引区间锁             |

# 分布式系统的挑战

**面向容错进行设计**是对分布式系统软件的基本要求。*基于不可靠的组件构建可靠系统* 是工程领域并不罕见的思想，如：
1.  纠错码能够容忍信道中偶尔一两个比特的误传。
2.  IP 层不可靠，但 TCP 层却基于 IP 层提供了相对可靠的传输保证。

## 异步的网络

**所有机器不共享资源（如内存、磁盘），通信的唯一途径就是网络**。互联网和数据中心（多是以太网）的内部网络多是**异步封包网络**（**asynchronous packet networks**）。反正有各种可能性导致你在超时时间内收不到回应，而你不知道对方情况如何。

### 故障检测

如果你想确定某个请求**确实成功**了，只能在应用层进行**显式确认**。在某些特定的场景下，你可以通过一些旁路信号，来获取一些信息：
-   **操作系统通知**。如果你能触达服务所在机器，但发现没有进程在监听预期端口（比如对应服务进程挂了），操作系统会通过发送 RST 或 FIN 包来**关闭 TCP 连接**。但是如果对端节点在处理你的请求时整个宕机了，就很难得知你请求的具体处理进度。
-   **daemon 脚本通知**。可以通过一些 **守护脚本**，在本机服务进程死掉之后，主动**通知其他节点**。来避免其他节点通过发送请求超时来判断此节点的服务进程不可用。当然这前提是，服务进程挂了，但所在节点没挂。
-   **数据链路层面**。如果你是管理员，并且能访问到你数据中心的网络交换机，可以在数据链路层判断远端机器是否宕机。
-   **IP 不可达**。如果路由器发现你要发送请求的 IP 地址不可达，它会直接回你一个 ICMP 不可达包。但路由器也并不能真正判断是否该机器不可用了。

### 网络拥塞与数据包排队
1.  **去程网络排队**。如果多个节点试图将数据包同时发给一个目的端，则交换机得将他们**排队**以逐个送达目的端。如果流量进一步增大，超过交换机的处理能力，则其可能会随机进行丢包。
2.  **目的机器排队**。当数据包到达目的端时，如果目标机器 CPU 负载很高，操作系统会将进来的数据包进行排队，直到有时间片分给他们。
3.  **虚拟机排队**。在虚拟化环境中，由于多个虚拟机共用物理机，因此经常会**整体让出 CPU 一段时间**的情况，等待期间是不能处理任何外部请求的，又会进一步给网络请求的排队时延增加变数。
4.  **TCP 流控**。TCP 流量控制会限制发送方的发送速度，以避免网络过载或者目的端过载（本机排队）。

### 同步网络

计网学过，**电路交换（circuit switching）** 是同步的。问题是需要**预留**带宽，利用率低，且很难应对互联网中无处不在的**突发流量**（bursty traffic）。

## 不可靠的时钟

当代的计算机通常支持两类时钟：**日历时钟**（time-of-day clock）和**单调时钟**（monotonic clock）。前者常用于物理时间点需求，后者常用于计算时间间隔。

### 时间同步与精度问题

对于日历时钟来说，由于自身石英钟计时不够精确，为了能够正常使用，需要定时与 NTP 服务器或者其他**可信时钟源**进行**同步**。但是校准的 NTP 服务都不是完全可靠的：
1. 单机的硬件时钟都不是很精确，会发生**漂移**（drift，走的快或者慢）
2. 网络延迟
3. **闰秒** 的存在，会导致一分钟可能有 59s 或者 61s，对一些系统不兼容
4. 虚拟机中，其**物理时钟是虚拟化**出来的，让出内核的等待期存在跳变

### 时间戳以定序

一种方案是将时钟的读数视为**具有置信区间**的**时间范围**。

快照隔离的实现通常需要一个**全局自增的事务 ID**。在多机协作下，必须要反应**因果性**（如当事务 B 读到事务 A 写的内容时，事务 B 的事务 ID 就需要比事务 A 大）。我们使用**置信区间，确定时间无交集**，再进行处理。`BOTH earlist < last`）

### 进程停顿问题

使用**租约**（lease）限定主副本的掌权时间，保证**任意时刻只有一个副本可以持有改租约**。但是，如果由于时钟跳变导致线程停顿了 15 秒，前面的租约判定失效了。比如：
1. GC stop the world -> 将GC暂停视为节点临时离线，通知其他节点接管
2. 虚拟机被挂起、在线迁移
3. 操作系统上下文切换、换页、进程暂停信号 -> 实时操作系统（RTOS）

## 消息的真假？

### 多数派定义
任何节点都没法**独自断言**其自身当前状态。大部分分布式算法会基于一个**法定人数**（*quorum*），即让所有节点进行投票：**任何决策都需要达到法定人数才能生效，以避免对单节点的依赖**。

在很多场景下，系统会要求某些东西全局唯一，比如：
1.  每个数据库分片都有唯一的领导者，避免脑裂
2.  只有一个事务或者客户端允许持有某资源的锁，以避免并发写入或者删除

### Fencing 令牌
![image.png](http://img.070077.xyz/20230116043229.png)

在锁服务每次授予锁或者租约时，会附带给一个**防护令牌**（fencing token），拒绝携带过期令牌的请求。即，*关联了ID的锁服务*。

**客户端不能独自确定其对资源的独占性**。需要在服务端对所有客户端的情况做一个二次核验。（默认：客户端都是老六）

### 拜占庭错误

系统中的节点有“说谎”（发送任意错误的的或者损坏的信息）的可能性，如区块链。这种行为称为**拜占庭故障**（Byzantine fault），在具有拜占庭故障的环境中达成共识也被称为**拜占庭将军问题**（Byzatine Generals Problem）。

这个的解决需要软硬兼施，比较复杂，在此就不展开了。

为软件加上一些对**弱谎言**（week forms of lying）的简单防护机制仍然很有用，如**应用层的校验字段**。

## 系统模型与现实

对于时间的假设，有三种系统模型很常用：
1.  **同步模型（synchronous model）**。假设**网络延迟**、**进程停顿**和**时钟错误**都是**有界**的。
2.  **半同步模型（partial synchronous）**。意思是**大部分时间里**，网络延迟、进程停顿和时钟漂移都是有界的，只有偶尔，他们会超过界限。这是一种比较真实的模型。任何关于时限的假设都有可能被打破，一旦出现出现异常现象，我们需要做好最坏的打算。
3.  **异步模型（Asynchronous model）**。算法不能对时间有任何假设，甚至时钟本身都有可能不存在（超时根本没有意义）。

除时间问题，我们还需要对节点故障进行抽象。针对节点，有三种常用系统模型：
1.  **宕机停止故障（Crash-stop faults）**。节点只会通过崩溃的方式宕机，即某个时刻可能会突然宕机无响应，并且之后永远不会再上线。
2.  **宕机恢复故障（Crash-recovery faults）**。节点可能会在任意时刻宕机，但在宕机之后某个时刻会重新上线，但恢复所需时间我们是不知道的。我们假设节点的稳定存储中的数据在宕机前后不会丢失，但内存中的数据会丢失。
3.  **拜占庭（任意）故障（Byzantine (arbitrary) faults）**。我们不能对节点有任何假设，包括宕机和恢复时间，包括善意和恶意。

对于分布式系统算法，如果能够应对该模型下的所有可能出现的情况，并且时刻满足其**约束性质**，则我们称该算法是**正确**的。此外有两类不同的属性衡量：
1. **安全性 safety**，通俗的可以理解为**没有坏事发生**（nothing bad happens）
2. **存活性 liveness**，可以理解为**好的事情最终发生了**（something good eventually happens），如最终一致性确实最终一致了。

我们通常会比较关注安全性，在**系统模型**可能触到的各种情况下，安全性**都必须**满足。

# 一致性与共识

共识协议可以让多机对某个**确定**的**操作序列（日志）** 达成共识，进而对系统的任意状态达成共识。

> **事务隔离级别**是为了解决并发所引起的竞态条件，**分布式一致性**是处理由于多副本间延迟和故障所引入的数据同步问题

## 线性一致性/可线性化

可线性化的基本想法是：让一个系统看起来只有一个数据副本，且所有的操作都是原子的，**是单个对象的最新值保证**。

### 如何满足可线性化

Q：读请求时间条与写请求有交集，因此读请求的返回值不确定。我们无从判断在数据库端，读操作和写操作的具体先后关系，因为这些读请求和写请求都是**并发的**。**在写请求持续期间，读客户端可能会看到不断交替的新旧值**。

A：线性一致性要求所有操作标记组成序列是**永远向前的，一旦我们写入或者读取到某值，所有稍后的读请求都能看到该值（坍缩）**，直到有人再次将其改写。

![image.png](http://img.070077.xyz/20230116044835.png)

### 线性化的依赖条件

Q：选主为避免脑裂，所有节点都必须同意哪个节点持有相应资源，必须满足可线性化。

A：**唯一性约束**。可使用共识算法，参考下文。

### 多通道的时间依赖

![image.png](http://img.070077.xyz/20230116045408.png)

Q：上图中由于存在**分布式存储写请求和消息传递**两条时间通道，存在竞争条件，需要线性化的**就近性保证**。

A：可以参考读写一致性（时间戳）的实现，用复杂性换线性一致性。

### 实现线性化系统

回顾下第五章的几种多副本模型，逐一考察：
-    **单主模型**（Single-leader replication，*potentially* linearizable） 在一个单主模型的系统中，主副本服务于写请求，其他副本负责备份。如果我们让读取也走主副本，或者使用同步更新从副本的策略，则该系统**有可能**满足线性一致性。
  1. 首先我们得确切知道哪一个是主副本。如果这个**自以为是的主节点**（delusional leader）继续提供服务，则系统很可能会违反线性一致性。
  2. 如果使用异步同步策略，节点宕机可能甚至会丢数据，从而不仅违反线性一致性，也违反了可持久性。
-   **共识算法**（Consensus algorithms，*linearizable*） 有一些共识算法，看起来与单主模型类似，但这些共识协议有一些阻止脑裂和过期副本的手段。由于这些额外细节，共识算法可以实现安全的线性一致性存储。如 Zookeeper 和 etcd 
-   **多主模型**（Multi-leader replication，*not* linearizable） 由于可以同时在多个节点上处理写入，并且异步同步写入数据，使用多主模型的系统通常不是线性一致的。由于上述原因，这种系统可能会产生需要手动解决的写入冲突。
-   **无主模型**（Leader replication， *probably not* linearizable） 前面提到，w+r > n 在一些corner case下，并不是强一致的（也取决于你如何定义强一致性）

### 可线性化的代价

![image.png](http://img.070077.xyz/20230116050449.png)

在使用多主模型的数据库中，在上图情形下，由于向其他数据中心的数据传输是异步的，每个数据中心仍能正常工作，只是由于数据中心间网络的问题，所有数据同步都被排队了起来，待到网络恢复就会重新发出。当然该客户端可以无视该中断，直接从从数据中心的从副本进行读取，但其读到的内容可能是过期的（主副本接受了新的写入），也因此不满足线性一致。如果应用层要求线性一致的读写，则**数据中心间的网络中断会造成服务的不可用**。

> CAP：**如果系统不提供线性一致性，就可以对网络故障更加鲁棒**。
> 现代多核CPU上的内存就是非线性化的（每个核有独立的cache和寄存器），CAP理论已不适用于当前的多核-内存一致性模型。放开线性化的原因是性能（内存屏障）而不是容错。有证明，如果想要满足线性化，那么读写请求的响应时间至少要和网络延迟成正比。

## 保序

因果将顺序施加于**事件**（event）：
1.  先有因，后有果
2.  先有消息发送，然后该消息被收到
3.  先有问题，后有答案

如果一个系统遵循因果约束，则我们称其为**因果一致的**（causally consistent）。
回顾一些因果例子：
1. 一致前缀读。（观察者的读取来自不同partition，顺序与实际请求先后相反）
2. 多主模型网络延迟导致条目不一致
3. **发生于之前**（happened before）是因果性的另一种表现，并发则无因果联系。
4. **读倾斜（不可重复读）**
5. 在**可串行的快照隔离级别**(SSI)下，通过追踪事务间的因果依赖（即读写数据集依赖）来检测写倾斜。
6. 未线性化坍缩：一旦某个读返回新值，之后所有读（不论分区）都必须返回新值
7. 多通道的时间依赖

### 因果序非全序

全序和偏序的区别也体现在不同强度**数据库一致性模型**上：
-   **线性一致性**（Linearizability）：对于任意两个操作，我们总是可以确定其发生的先后关系，满足全序关系，对应事件的时间线
-   **因果一致性**（Causality）。如果我们无从判定两个操作的先后关系，则称之为**并发关系**。换言之，如果两个事件因果相关，则可排序；另外一类操作是并发的，则不可比。也即，因果性定义了一种**偏序**（partial order）关系。

**因果依赖是应用层定义的，系统层较难完全识别**，版本向量技术、事务追踪等可以推广为通用的部分解决方案。

线性一致性是因果一致性的**充分（implies）条件**。从事件拓扑来看，可以认为**因果一致性**（DAG） ⇒ **线性一致性**（经过所有事件点的单向路径）

### 序列号定序法

**如果操作 A 发生在 B 之前，则 A 获取到的序列号比 B 小**。并发操作获取到的序列号顺序不确定。（本质上是一种全序）
**单主模型**：主节点上**操作日志的追加顺序**确定了一个对所有操作的全序，且满足操作发生的因果关系。主节点可以为每条日志按顺序关联一个全局递增的序列号，副本按此序应用。

**非单主模型**：因为**不同节点上序列号的增长速率很难完全同步**、**物理时间戳存在多机时钟偏差**，不能够很好地捕捉跨节点的操作因果关系。提出一种相对简洁的**Lamport 时间戳**法（但不是充分必要的，即不能通过两个 Lamport 时间戳的大小来判断其是有因果关系还是并发关系）：

### Lamport 时间戳定序

每个节点有一个唯一的 **id** 和一个记录操作计数器，Lamport 时间戳是上述两者组成的二元组：`(counter, node ID)`。显然是唯一的，可比：具有较大 counter 的时间戳较大，则counter 相同，具有较大 node ID 的时间戳较大。让 Lamport 时间戳能够满足因果一致性的核心点在于：**每个节点和客户端都会让 counter 追踪当前所看到（包括本机的和通信的）的最大值。当节点看到请求或者回复中携带的 counter 值比自己大，就会立即用其值设置本地 counter**。

但是还有一种case：**唯一性约束**。如两个客户端创建相同用户名的账户，**当你拿到系统中所有的账户创建操作后，你才可以比较他们的时间戳。**（只有一方，何谈比较？）也就是说，**只有在收集到系统中所有操作之后，才能真正确定所有操作的全序**。单个节点并不能**立即独自**判断该请求成功还是失败。

在此case中，为判断其他节点是否【收到同名账户的创建请求，并且获得了较小的时间戳】，又想考虑高可用（其他节点宕机或者网络故障时，依旧可提供服务）下：*这些未知节点的操作可能被插入全序到不同位置，不能确定最终的事件的全序，我不知道能否执行请求*。因此**仅为所有时间进行全局定序是不够的，你还需要知道该定序何时完成(收敛)。**

### 全序广播

全序广播是一种多个节点间交换消息的协议，在分布式系统中常用于对多条执行事件线进行定全序。它要求系统满足两个安全性质：
1. **可靠交付**。如果一个节点收到了消息，则系统内所有相关节点都要收到该消息。
2. **全序交付**。每个节点接收到消息的顺序一致。(**当收到消息时，其顺序已经确定**)

还可以从另外一个角度来理解全序广播——用来写日志（比如复制日志、事务日志或者写前日志）：**投递消息就像追加日志**。由于所有节点都会按照同样的顺序发送消息，则所有节点在读取日志的时候也会得到同样的消息序列。像 Zookeeper 和 etcd 等共识服务都实现了全序广播算法。

> 全序广播是**异步的**：系统保证以同样的**顺序**交付消息，但并不保证消息的交付**时刻**（即，有的消息接收者间可能存在着滞后）。
> 与之相对，线性一致性是一种**新鲜度保证**：读取一定能看到最新成功的写。

我们来看几个例子：

1. 实现 Fencing 令牌
   每个上锁请求可以作为消息追加到日志中，依据其追加到日志中的顺序，所有请求可以被自然地编号。由于这个序列号是单调递增的，便可以充当防护令牌。在 Zookeeper 中，这个序列号便是 zxid。

2. 实现线性一致性存储（上面创建账户的例子）
   对每一个可能的用户名，我们使用一个支持 CAS 操作的线性寄存器，使用某个用户名创建账户时，`CAS(old=null, new=account-id)`。
   使用全序广播系统作为**日志追加服务**，可实现这样的*原子CAS寄存器*：
   - 服务中追加一个带有**某用户名的消息条目**，表明你想使用该用户名。
   - （由于全序广播是异步的，需要等待同步）不断读取日志，直到能够读到刚才你追加的消息条目。
   - 检查所有想要使用该用户名的消息，这时你可能会得到多条消息，当且仅当你当初写下的消息在第一条，则你是成功的。
> 因为异步，尽管该方式能够提供线性化的写入，却不能保证线性化的读取。如果想让读取也变得可线性化，有几种做法：
> -   让读取也走日志，即通过追加消息的方式将读取顺序化，然后当读取请求所在节点**收到**这条读取日志时才去真正的去读。则消息在日志中的位置定义了整个时间序列中读取真正发生的时间点。（etcd 中的法定读取就是用的类似的做法）
> -   如果日志服务允许查询最新日志的位置，则可以在请求到来时，获取当时最新位置，然后不断查询日志看是否已经跟到最新位置。如果跟到了，就进行读取。（这是 Zookeeper 中 sync() 操作的背后的原理）
> -   可以将读取路由到写入发生的那个节点，或者与写入严格同步的节点，以保证能够读到最新的内容。（这种技术用于**链式复制** chain replication 中）

3. 使用线性一致性的存储实现全序广播
   实际上我们是可以反向实现上面的case的...
   最简单的方法，假设我们有一个整数寄存器，并提供 increment-and-get 原子操作，来实现一个**分布式系统的"TCP"**：
   - 对于每一个发给全序广播系统的消息，使用整数寄存器 increment-and-get 操作关联一个序列号
   - 将消息发送给所有节点（重试任何丢失的消息）。每个节点接收到消息后利用序列号顺序对外交付消息，并通过ACK确认。
> 和 Lamport 时间戳不同，从线性化的寄存器中获取的数字是**连续的，非跳跃的**。如某节点交付了消息 4 后，收到了消息 6，但不能立即交付，而需要等待消息 5 的到来。但在 Lamport 时间戳系统中则非如此——这（是否连续）**也是全序广播和时间戳顺序的核心不同**。

但其实有个老问题，怎么得到一个高可用的线性序列生成器？可以证明，一个线性的 CAS 寄存器和全序广播都**等价于共识协议**（equivalent to consensus）。

## 分布式事务与共识协议 🌟
